{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/122004/122004/blob/main/Copy_of_habermas_machine_newspeak_event_250118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEWU3BUzd3z6"
      },
      "source": [
        "# Exploring Deliberative Democracy with the Habermas Machine\n",
        "\n",
        "This colab contains the code for running a version of the Habermas Machine based on a series of prompted Gemini models, for use at the Newspeak House event on Jan 18, 2025.\n",
        "\n",
        "The purpose of this notebook is to demonstrate the workings of the Habermas Machine, using publicly accessible models rather than the custom fine-tuned model. The Habermas Machine was introduced in this paper:\n",
        "\n",
        "Tessler, M. H., Bakker, M. A., Jarrett D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, J., Collins, T., Parkes, D. C., Botvinick, M., and Summerfield, C. \"AI can help humans find common ground in democratic deliberation.\" *Science*. (2024). [[url]](https://www.science.org/stoken/author-tokens/ST-2196/full)\n",
        "\n",
        "Colab by Michiel Bakker (miba@google.com) and MH Tessler (mhtessler@google.com)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eRjrkiagYWh"
      },
      "source": [
        "# Setup and importing packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xksSutDwG7z8"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import abc\n",
        "import enum\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import textwrap\n",
        "import time\n",
        "from collections.abc import Collection, Mapping, Sequence\n",
        "from typing import NamedTuple\n",
        "\n",
        "from IPython.display import Javascript, Markdown\n",
        "\n",
        "from typing_extensions import override\n",
        "\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "\n",
        "from IPython.display import HTML\n",
        "shell = get_ipython()\n",
        "\n",
        "def adjust_font_size():\n",
        "  display(HTML('''<style>\n",
        "    body {\n",
        "      font-size: 14px;\n",
        "    }\n",
        "  '''))\n",
        "\n",
        "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
        "  shell.events.register('pre_execute', adjust_font_size)\n",
        "\n",
        "# Adjust font size dynamically\n",
        "def change_input_font_size(size=20):\n",
        "    display(Javascript(f\"\"\"\n",
        "        var elements = document.getElementsByClassName('input');\n",
        "        for (var i = 0; i < elements.length; i++) {{\n",
        "            elements[i].style.fontSize = '{size}px';\n",
        "        }}\n",
        "    \"\"\"))\n",
        "\n",
        "change_input_font_size(40)  # Adjust the size as needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LLM Client utils for colab.\n",
        "\n",
        "def truncate(\n",
        "    string: str,\n",
        "    *,\n",
        "    max_length: int = sys.maxsize,\n",
        "    delimiters: Collection[str] = (),\n",
        ") -> str:\n",
        "  \"\"\"Truncates a string to a maximum length up to a delimiter.\n",
        "\n",
        "  Args:\n",
        "    string: String to truncate\n",
        "    max_length: Maximum length of the string.\n",
        "    delimiters: Delimiters that must not be present in the truncated string.\n",
        "\n",
        "  Returns:\n",
        "    The longest prefix of string that does not exceed max_length and does not\n",
        "    contain any delimiter.\n",
        "  \"\"\"\n",
        "  truncated = string[:max_length]\n",
        "  for delimiter in delimiters:\n",
        "    truncated = truncated.split(delimiter, 1)[0] + delimiter\n",
        "  return truncated"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D4jrvG_KDVok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base class for LLM clients.\n",
        "\n",
        "DEFAULT_TEMPERATURE = 0.8\n",
        "DEFAULT_TERMINATORS = ()\n",
        "DEFAULT_TIMEOUT_SECONDS = 60\n",
        "# We truncate the response if we detect the terminator string before the max\n",
        "# tokens so we set a high default value for max tokens.\n",
        "DEFAULT_MAX_TOKENS = 4096\n",
        "\n",
        "class LLMClient(abc.ABC):\n",
        " \"\"\"Language model client base class.\"\"\"\n",
        "\n",
        " @abc.abstractmethod\n",
        " def sample_text(\n",
        "     self,\n",
        "     prompt: str,\n",
        "     *,\n",
        "     max_tokens: int = DEFAULT_MAX_TOKENS,\n",
        "     terminators: Collection[str] = DEFAULT_TERMINATORS,\n",
        "     temperature: float = DEFAULT_TEMPERATURE,\n",
        "     timeout: float = DEFAULT_TIMEOUT_SECONDS,\n",
        "     seed: int | None = None,\n",
        " ) -> str:\n",
        "   \"\"\"Samples text from the model.\n",
        "\n",
        "   Args:\n",
        "     prompt: The input text that the model conditions on.\n",
        "     max_tokens: The maximum number of tokens in the response.\n",
        "     terminators: The response will be terminated before any of these\n",
        "       characters.\n",
        "     temperature: Model temperature.\n",
        "     timeout: Timeout for the request.\n",
        "     seed: Optional seed for the sampling. If None a random seed will be used.\n",
        "\n",
        "   Returns:\n",
        "     The sampled response (i.e. does not include the prompt).\n",
        "\n",
        "   Raises:\n",
        "     TimeoutError: if the operation times out.\n",
        "   \"\"\"\n",
        "   raise NotImplementedError('sample_text method is not implemented.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KxDNvWVoDjD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Language Model that uses Google AI Studio API.\n",
        "\n",
        "DEFAULT_SAFETY_SETTINGS = (\n",
        "   {\n",
        "       'category': 'HARM_CATEGORY_HARASSMENT',\n",
        "       'threshold': 'BLOCK_ONLY_HIGH',\n",
        "   },\n",
        "   {\n",
        "       'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
        "       'threshold': 'BLOCK_ONLY_HIGH',\n",
        "   },\n",
        "   {\n",
        "       'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
        "       'threshold': 'BLOCK_ONLY_HIGH',\n",
        "   },\n",
        "   {\n",
        "       'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
        "       'threshold': 'BLOCK_ONLY_HIGH',\n",
        "   },\n",
        ")\n",
        "\n",
        "class AIStudioClient(LLMClient):\n",
        " \"\"\"Language Model that uses AI Studio API.\"\"\"\n",
        "\n",
        " def __init__(\n",
        "     self,\n",
        "     model_name: str,\n",
        "     *,\n",
        "     safety_settings: Sequence[Mapping[str, str]] = DEFAULT_SAFETY_SETTINGS,\n",
        "     sleep_periodically: bool = False,\n",
        " ) -> None:\n",
        "   \"\"\"Initializes the instance.\n",
        "\n",
        "   Args:\n",
        "     model_name: which language model to use. For more details, see\n",
        "       https://aistudio.google.com/.\n",
        "     safety_settings: Gemini safety settings. For more details, see\n",
        "       https://ai.google.dev/gemini-api/docs/safety.\n",
        "     sleep_periodically: sleep between API calls to avoid rate limit.\n",
        "   \"\"\"\n",
        "   self._api_key = os.environ['GOOGLE_API_KEY']\n",
        "   self._model_name = model_name\n",
        "   self._safety_settings = safety_settings\n",
        "   self._sleep_periodically = sleep_periodically\n",
        "\n",
        "   genai.configure(api_key=self._api_key)\n",
        "   self._model = genai.GenerativeModel(\n",
        "       model_name=self._model_name,\n",
        "       safety_settings=safety_settings,\n",
        "   )\n",
        "\n",
        "   self._calls_between_sleeping = 10\n",
        "   self._n_calls = 0\n",
        "\n",
        " @override\n",
        " @retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
        " def sample_text(\n",
        "     self,\n",
        "     prompt: str,\n",
        "     *,\n",
        "     max_tokens: int = DEFAULT_MAX_TOKENS,\n",
        "     terminators: Collection[str] = DEFAULT_TERMINATORS,\n",
        "     temperature: float = DEFAULT_TEMPERATURE,\n",
        "     timeout: float = DEFAULT_TIMEOUT_SECONDS,\n",
        "     seed: int | None = None,\n",
        " ) -> str:\n",
        "   del timeout\n",
        "   del seed  # AI Studio does not support seeds.\n",
        "\n",
        "   self._n_calls += 1\n",
        "   if self._sleep_periodically and (\n",
        "       self._n_calls % self._calls_between_sleeping == 0):\n",
        "     print('Sleeping for 10 seconds...')\n",
        "     time.sleep(10)\n",
        "\n",
        "   sample = self._model.generate_content(\n",
        "       prompt,\n",
        "       generation_config=genai.GenerationConfig(\n",
        "           temperature=temperature,\n",
        "           max_output_tokens=max_tokens,\n",
        "           stop_sequences=terminators,\n",
        "       ),\n",
        "       safety_settings=self._safety_settings,\n",
        "       stream=False,\n",
        "   )\n",
        "   try:\n",
        "     # AI Studio returns a list of parts, but we only use the first one.\n",
        "     response = sample.candidates[0].content.parts[0].text\n",
        "   except ValueError as e:\n",
        "     print('An error occurred: ', e)\n",
        "     print(f'prompt: {prompt}')\n",
        "     print(f'sample: {sample}')\n",
        "     response = ''\n",
        "   return truncate(response, delimiters=terminators)"
      ],
      "metadata": {
        "id": "ubCBPRdqDmjQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base class for reward models.\n",
        "\n",
        "RankingResult = NamedTuple(\n",
        "   'RankingResult',\n",
        "   [\n",
        "       ('ranking', np.ndarray | None),\n",
        "       ('explanation', str | None),\n",
        "   ],\n",
        ")\n",
        "\n",
        "\n",
        "class BaseRankingModel(abc.ABC):\n",
        " \"\"\"Base class for reward models that rank multiple statements.\"\"\"\n",
        "\n",
        " @abc.abstractmethod\n",
        " def predict_ranking(\n",
        "     self,\n",
        "     llm_client: LLMClient,\n",
        "     question: str,\n",
        "     opinion: str,\n",
        "     statements: Sequence[str],\n",
        "     previous_winner: str | None = None,\n",
        "     critique: str | None = None,\n",
        " ) -> RankingResult:\n",
        "   \"\"\"Samples text from the model.\n",
        "\n",
        "   Args:\n",
        "     llm_client: LLM client to use.\n",
        "     question: Question that the citizen is responding to.\n",
        "     opinion: Text-based opinion of the citizen.\n",
        "     statements: Statements that are ranked.\n",
        "     previous_winner: The statement that won the previous round.\n",
        "     critique: Critique of the previous winner.\n",
        "\n",
        "   Returns:\n",
        "     A RankingResult tuple, consisting of:\n",
        "       - Array of rankings with dimensions: [num_statements,]. In this array\n",
        "         lower is better and the best candidate is thus given rank 0. For\n",
        "         example, an array [0, 1, 0] corresponds to the first citizen\n",
        "         preferring candidates 0 and 2 over candidate 1, while preferring\n",
        "         candidates 0 and 2 equally. If the model has an error, None is\n",
        "         returned.\n",
        "       - Explanation for the ranking (for example the raw output including\n",
        "         the chain-of-thought) or the error. None if there is no explanation.\n",
        "   \"\"\"\n",
        "   raise NotImplementedError('predict_ranking method is not implemented.')"
      ],
      "metadata": {
        "id": "IQE97PF3Dm2i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [HAS PROMPT] A ranking model that uses chain-of-thought reasoning.\n",
        "\n",
        "class COTRankingModel(BaseRankingModel):\n",
        " \"\"\"A ranking model that uses chain-of-thought reasoning to rank statements.\"\"\"\n",
        "\n",
        " @override\n",
        " def predict_ranking(\n",
        "     self,\n",
        "     llm_client: LLMClient,\n",
        "     question: str,\n",
        "     opinion: str,\n",
        "     statements: Sequence[str],\n",
        "     previous_winner: str | None = None,\n",
        "     critique: str | None = None,\n",
        "     num_retries_on_error: int = 5,\n",
        " ) -> RankingResult:\n",
        "   \"\"\"Ranks statements based on their length (see base class).\"\"\"\n",
        "   if previous_winner is None and critique is not None:\n",
        "     raise ValueError(\n",
        "         'If there is a previous_winner, there should be a critique.'\n",
        "     )\n",
        "   if len(statements) < 2:\n",
        "     raise ValueError('There should be at least two statements to rank.')\n",
        "\n",
        "   for i in range(num_retries_on_error):\n",
        "    prompt = _generate_rm_prompt(\n",
        "        question, opinion, statements, previous_winner, critique\n",
        "    )\n",
        "    response = llm_client.sample_text(prompt, terminators=['</answer>'])\n",
        "\n",
        "    ranking, explanation = _process_rm_model_response(response)\n",
        "\n",
        "    if len(ranking) != len(statements):\n",
        "      error_msg = 'INCORRECT_RANKING_LENGTH'\n",
        "      if explanation:\n",
        "        error_msg += f', Explanation: {explanation}'\n",
        "      return RankingResult(None, error_msg)\n",
        "    if ranking is not None and 'INCORRECT' not in explanation:\n",
        "      break\n",
        "   return RankingResult(ranking, explanation)\n",
        "\n",
        "\n",
        "def _generate_rm_opinion_critique_prompt(\n",
        "   question: str,\n",
        "   opinion: str,\n",
        "   statements: Sequence[str],\n",
        "   previous_winner: str,\n",
        "   critique: str,\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt for the LLM using opinion and critique.\"\"\"\n",
        " prompt = f\"\"\"As an AI assistant, your job is to rank these statements in the order that the participant would most likely agree with them, based on their opinion and critique to a summary statement from a previous discussion round. Use Arrow notation for the ranking, where \">\" means \"preferred to\". Ties are NOT allowed and items should be in descending order of preference so you can ONLY use \">\" and the letters of the statements in the ranking. Examples of valid rankings: B > A, D > A > C > B. B > C > A > E > D.\n",
        "\n",
        "Please think through this task step-by-step:\n",
        "\n",
        "1. Analyze the participant's opinion and critique, noting key points and sentiments.\n",
        "2. Analyze the critique to the summary statement from the previous discussion round.\n",
        "3. Compare each statement to the participant's opinion and critique, considering how well it aligns with or supports their view.\n",
        "4. Consider any nuances or implications in the statements that might appeal to or repel the participant based on their expressed opinion.\n",
        "5. Rank the statements accordingly using only \">\" and the letters of the statements.\n",
        "\n",
        "Provide your answer in the following format:\n",
        "<answer>\n",
        "[Your step-by-step reasoning and explanation for the ranking]\n",
        "<sep>\n",
        "[Final ranking using arrow notation]\n",
        "</answer>\n",
        "\n",
        "For example for five statements A, B, C, D and E a valid response could be:\n",
        "<answer>\n",
        "1. The participant's opinion emphasizes the importance of environmental protection and the need for immediate action to address climate change. The critique of the previous winner highlights that it failed to offer specific solutions.\n",
        "\n",
        "2. The critique emphasizes the need for concrete solutions to address climate change, indicating that the participant values action-oriented approaches.\n",
        "\n",
        "3. - Statement A directly addresses the urgency of climate action and proposes concrete steps, aligning with both the participant's opinion and critique.\n",
        " - Statements B and D acknowledge the seriousness of climate change but offer less concrete solutions. B focuses on global cooperation, while D emphasizes economic considerations.\n",
        " - Statement C downplays the urgency of climate change, contradicting the participant's stance.\n",
        " - Statement E completely opposes the participant's view by denying the existence of climate change.\n",
        "\n",
        "4.  The participant's emphasis on immediate action suggests a preference for proactive solutions and a dislike for approaches that downplay the issue or offer only abstract ideas.\n",
        "\n",
        "5. Based on this analysis, the ranking is: A > D > B > C > E\n",
        "\n",
        "<sep>\n",
        "A > D > B > C > E\n",
        "</answer>\n",
        "\n",
        "It is important to follow the template EXACTLY. So ALWAYS start with <answer>, then the explanation, then <sep> then only the final ranking and then </answer>.\n",
        "\n",
        "Below you will find the question, the participant's opinion, the statement from the previous round, and a critique of that statement. You will also find a list of statements to rank.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Participant's Opinion: {opinion}\n",
        "\n",
        "Statement from previous round: {previous_winner}\n",
        "\n",
        "Critique: {critique}\n",
        "\n",
        "Statements to rank:\n",
        "\"\"\"\n",
        " for i, statement in enumerate(statements):\n",
        "   letter = chr(ord('A') + i)  # A, B, C, D, etc.\n",
        "   try:\n",
        "     statement = (\n",
        "         statement.strip().strip('').strip('\"\"').strip('\\n').strip()\n",
        "     )\n",
        "   except Exception as exc:\n",
        "     raise ValueError(f'Issue with statement: {statement}') from exc\n",
        "   prompt += f'{letter}. {statement}\\n'\n",
        "\n",
        " return prompt.strip()\n",
        "\n",
        "\n",
        "def _generate_rm_opinion_only_prompt(\n",
        "   question: str,\n",
        "   opinion: str,\n",
        "   statements: Sequence[str],\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt for the LLM using only the opinion.\"\"\"\n",
        " prompt = f\"\"\"\n",
        "Task: As an AI assistant, your job is to rank these statements in the order that the participant would most likely agree with them, based on their opinion. Use Arrow notation for the ranking, where \">\" means \"preferred to\". Ties are NOT allowed and items should be in descending order of preference so you can ONLY use \">\" and the letters of the statements in the final ranking. Examples of valid final rankings: B > A, D > A > C > D. B > C > A > E > D.\n",
        "\n",
        "Please think through this task step-by-step:\n",
        "\n",
        "1. Analyze the participant's opinion, noting key points and sentiments.\n",
        "2. Compare each statement to the participant's opinion, considering how well it aligns with or supports their view.\n",
        "3. Consider any nuances or implications in the statements that might appeal to or repel the participant based on their expressed opinion.\n",
        "4. Rank the statements accordingly using only \">\" and the letters of the statements.\n",
        "\n",
        "Provide your answer in the following format:\n",
        "<answer>\n",
        "[Your step-by-step reasoning and explanation for the ranking]\n",
        "<sep>\n",
        "[Final ranking using arrow notation]\n",
        "</answer>\n",
        "\n",
        "For example for five statements A, B, C, D and E a valid response could be:\n",
        "<answer>\n",
        "1. The participant's opinion emphasizes the importance of environmental protection and the need for immediate action to address climate change.\n",
        "\n",
        "2. - Statement A directly addresses the urgency of climate action and proposes concrete steps, aligning with the participant's opinion.\n",
        "  - Statements B and D acknowledge the seriousness of climate change but offer less concrete solutions. B focuses on global cooperation, while D emphasizes economic considerations.\n",
        "  - Statement C downplays the urgency of climate change, contradicting the participant's stance.\n",
        "  - Statement E completely opposes the participant's view by denying the existence of climate change.\n",
        "\n",
        "3.  The participant's emphasis on immediate action suggests a preference for proactive solutions and a dislike for approaches that downplay the issue or offer only abstract ideas.\n",
        "\n",
        "4. Based on this analysis, the ranking is: A > D > B > C > E\n",
        "\n",
        "<sep>\n",
        "A > D > B > C > E\n",
        "</answer>\n",
        "\n",
        "It is important to follow the template EXACTLY. So ALWAYS start with <answer>, then the explanation, then <sep> then only the final ranking and then </answer>.\n",
        "\n",
        "Below you will find the question and the participant's opinion. You will also find a list of statements to rank.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Participant's Opinion: {opinion}\n",
        "\n",
        "Statements to rank:\n",
        "\"\"\"\n",
        " for i, statement in enumerate(statements):\n",
        "   letter = chr(ord('A') + i)  # A, B, C, D, etc.\n",
        "   try:\n",
        "     statement = (\n",
        "         statement.strip().strip('').strip('\"\"').strip('\\n').strip()\n",
        "     )\n",
        "   except Exception as exc:\n",
        "     raise ValueError(f'Issue with statement: {statement}') from exc\n",
        "   prompt += f'{letter}. {statement}\\n'\n",
        "\n",
        " return prompt.strip()\n",
        "\n",
        "\n",
        "def _generate_rm_prompt(\n",
        "   question: str,\n",
        "   opinion: str,\n",
        "   statements: Sequence[str],\n",
        "   previous_winner: str | None = None,\n",
        "   critique: str | None = None,\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt for the LLM.\"\"\"\n",
        " if previous_winner is None:\n",
        "   return _generate_rm_opinion_only_prompt(\n",
        "       question, opinion, statements\n",
        "   )\n",
        " else:\n",
        "   return _generate_rm_opinion_critique_prompt(\n",
        "       question, opinion, statements, previous_winner, critique\n",
        "   )\n",
        "\n",
        "\n",
        "def _check_response_format(response: str) -> bool:\n",
        " \"\"\"Checks if the response is in a correct format with <answer> and <sep>.\n",
        "\n",
        " Args:\n",
        "   response: The model's raw response\n",
        "\n",
        " Returns:\n",
        "   bool: True if the format is correct, False otherwise\n",
        " \"\"\"\n",
        " pattern = r'<answer>\\s*.*?\\s*<sep>\\s*.*?\\s*</answer>'\n",
        " return bool(re.search(pattern, response, re.DOTALL))\n",
        "\n",
        "\n",
        "def _check_arrow_format(arrow_ranking):\n",
        " \"\"\"Checks if the arrow ranking format is correct.\n",
        "\n",
        " Args:\n",
        "   arrow_ranking: The arrow ranking string (eg A > B > C)\n",
        "\n",
        " Returns:\n",
        " bool: True if the format is correct, False otherwise\n",
        " \"\"\"\n",
        " if len(arrow_ranking) < 3:\n",
        "   return False\n",
        "\n",
        " # Remove whitespace and replace multiple spaces with single spaces.\n",
        " arrow_ranking = re.sub(r'\\s+', ' ', arrow_ranking.strip())\n",
        "\n",
        " # Remove spaces around '>' and '=' symbols.\n",
        " arrow_ranking = re.sub(r'\\s*(>|=)\\s*', r'\\1', arrow_ranking)\n",
        "\n",
        " # Check if the ranking contains only allowed characters.\n",
        " if not re.match(r'^[A-Z>=]+$', arrow_ranking):\n",
        "   return False\n",
        "\n",
        " # Check for consecutive '>' symbols, '=' at the start/end,\n",
        " # or '=' immediately before '>'.\n",
        " if (\n",
        "     '>>' in arrow_ranking\n",
        "     or arrow_ranking.startswith('=')\n",
        "     or arrow_ranking.endswith('=')\n",
        "     or '=>' in arrow_ranking\n",
        " ):\n",
        "   return False\n",
        "\n",
        " # Split by '>' and check each group\n",
        " groups = arrow_ranking.split('>')\n",
        "\n",
        " if len(groups) < 1:\n",
        "   return False\n",
        "\n",
        " seen_letters = set()\n",
        " for group in groups:\n",
        "   # Check if the group is empty.\n",
        "   if not group:\n",
        "     return False\n",
        "   # Check if each group contains only unique letters separated by '='.\n",
        "   letters = group.split('=')\n",
        "   if len(letters) != len(set(letters)):\n",
        "     return False\n",
        "   # Check if any letter in this group has been seen before.\n",
        "   if any(letter in seen_letters for letter in letters):\n",
        "     return False\n",
        "   seen_letters.update(letters)\n",
        "\n",
        " return True\n",
        "\n",
        "def _extract_arrow_ranking(text: str) -> str | None:\n",
        " \"\"\"Extracts the arrow ranking from a given string.\n",
        "\n",
        " Args:\n",
        "   text: The input string containing the arrow ranking.\n",
        "\n",
        " Returns:\n",
        "   The extracted arrow ranking or None if not found.\n",
        " \"\"\"\n",
        " # Regular expression to match a full arrow ranking pattern\n",
        " match = re.search(r'\\b([A-Z](?:\\s*(?:>|=)\\s*[A-Z])*)\\b', text)\n",
        "\n",
        " if match:\n",
        "   return match.group(1).replace(' ', '')  # Removes any extra spaces\n",
        " else:\n",
        "   return None\n",
        "\n",
        "def _process_rm_model_response(response: str) -> tuple[np.ndarray | None, str]:\n",
        " \"\"\"Processes the model's response, extract the explanation and arrow ranking.\n",
        "\n",
        " Args:\n",
        "   response: The raw model response.\n",
        "\n",
        " Returns:\n",
        "   A tuple of:\n",
        "   - np.ndarray: The arrow ranking if it is correct, None otherwise.\n",
        "   - str: The explanation if it is correct, \"INCORRECT_TEMPLATE\" if the\n",
        "   response format is incorrect, or \"INCORRECT_ARROW_RANKING\" if the arrow\n",
        "   ranking is incorrect.\n",
        " \"\"\"\n",
        " if _check_response_format(response):\n",
        "   match = re.search(\n",
        "       r'<answer>\\s*(.*?)\\s*<sep>\\s*(.*?)\\s*</answer>', response, re.DOTALL\n",
        "   )\n",
        "   if match is None:\n",
        "     return None, f'INCORRECT_TEMPLATE: {response}'\n",
        "   else:\n",
        "     explanation = match.group(1).strip()\n",
        "     arrow_ranking = _extract_arrow_ranking(\n",
        "         match.group(2).strip())\n",
        " else:\n",
        "   # Backup as it sometimes returns \"final ranking:\" in a different format.\n",
        "   match = re.search(r'(?i)final ranking:\\s*(.*)', response)\n",
        "   if match is None:\n",
        "     return None, f'INCORRECT_TEMPLATE: {response}'\n",
        "   else:\n",
        "     explanation = response\n",
        "     arrow_ranking = _extract_arrow_ranking(match.group(1))\n",
        "\n",
        " if arrow_ranking is None or not _check_arrow_format(\n",
        "     arrow_ranking\n",
        " ):\n",
        "   # Check if the ranking is in the explanation.\n",
        "   arrow_ranking = _extract_arrow_ranking(\n",
        "       explanation.strip()\n",
        "   )\n",
        "   if arrow_ranking is None or not _check_arrow_format(\n",
        "       arrow_ranking\n",
        "   ):\n",
        "     return None, f'INCORRECT_ARROW_RANKING: {response}'\n",
        "\n",
        " # Convert arrow ranking to numpy array.\n",
        " elements = re.findall(r'[A-Z]', arrow_ranking)\n",
        " unique_elements = sorted(set(elements))\n",
        " ranking_dict = {element: 0 for element in unique_elements}\n",
        "\n",
        " groups = arrow_ranking.split('>')\n",
        " for rank, group in enumerate(groups):\n",
        "   tied_elements = group.strip().split('=')\n",
        "   for element in tied_elements:\n",
        "     ranking_dict[element.strip()] = rank\n",
        "\n",
        " result = np.array([ranking_dict[element] for element in unique_elements])\n",
        "\n",
        " return result, response"
      ],
      "metadata": {
        "id": "eV4pk5N-Dm5Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base social ranking method for aggregating ranked preferences.\n",
        "\n",
        "RANKING_MOCK = -1\n",
        "\n",
        "SocialRankingResult = NamedTuple(\n",
        "   'SocialRankingResult',\n",
        "   [\n",
        "       ('social_ranking', np.ndarray),\n",
        "       ('untied_social_ranking', np.ndarray),\n",
        "   ],\n",
        ")\n",
        "\n",
        "class Base(abc.ABC):\n",
        " \"\"\"Social ranking base class.\"\"\"\n",
        "\n",
        " def __init__(self, tie_breaking_method: 'TieBreakingMethod'):\n",
        "   \"\"\"Initialize the Base class.\n",
        "\n",
        "   Args:\n",
        "       tie_breaking_method: Method that is used to break ties.\n",
        "   \"\"\"\n",
        "   self._tie_breaking_method = tie_breaking_method\n",
        "\n",
        " @abc.abstractmethod\n",
        " def aggregate(\n",
        "     self,\n",
        "     rankings: np.ndarray,\n",
        "     seed: int | None = None,  # TODO(miba): Input rng instead of seed.\n",
        " ) -> SocialRankingResult:\n",
        "   \"\"\"Aggregates a set of rankings into a single social ranking.\n",
        "\n",
        "   Args:\n",
        "     rankings: Array of batched rankings with dimensions: [num_citizens,\n",
        "       num_candidates]. In this array lower is better and the best candidate\n",
        "       is thus given rank 0. For example, an array [[1, 0], [0, 0]] corresponds\n",
        "       to the first citizen preferring candidate 1 over candidate 0 while the\n",
        "       second citizen has no preference for either candidate over the other.\n",
        "     seed: Optional seed for tie breaking.\n",
        "\n",
        "   Return:\n",
        "     A tuple of:\n",
        "     - An array with the aggregated social rank for each candidate.\n",
        "       If B>C>A, the array will be [2, 0, 1]. The array has dimensions:\n",
        "       [num_candidates,]. In this array, ties are allowed and there can be\n",
        "       multiple potential winners.\n",
        "     - Untied aggregated social rank. The ranks are untied using the\n",
        "       `tie_breaking_method`. If there were no ties in the social ranking,\n",
        "       this just returns the same ranking.\n",
        "   \"\"\"\n",
        "   raise NotImplementedError('Base class is not implemented.')"
      ],
      "metadata": {
        "id": "P27TiB6VDm8Q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Social ranking method that implements the Schulze method.\n",
        "\n",
        "class Schulze(Base):\n",
        " \"\"\"Schulze social ranking method (Schulze, M. 2011).\n",
        "\n",
        " We follow the steps from https://electowiki.org/wiki/Schulze_method.\n",
        " \"\"\"\n",
        "\n",
        " @override\n",
        " def aggregate(\n",
        "     self,\n",
        "     rankings: np.ndarray,\n",
        "     seed: int | None = None,\n",
        " ) -> SocialRankingResult:\n",
        "   \"\"\"Aggregates rankings into a single social ranking and unties the ranking.\n",
        "\n",
        "   Args:\n",
        "     rankings: Array of batched rankings with dimensions: [num_citizens,\n",
        "       num_candidates]. In this array lower is better and the best candidate\n",
        "       is thus given rank 0. For example, an array [[1, 0], [0, 0]] corresponds\n",
        "       to the first citizen preferring candidate 1 over candidate 0 while the\n",
        "       second citizen has no preference for either candidate over the other.\n",
        "     seed: Random seed that is used to break ties.\n",
        "\n",
        "   Returns:\n",
        "     A tuple of:\n",
        "     - An array with the aggregated social rank for each candidate.\n",
        "       If B>C>A, the array will be [2, 0, 1]. The array has dimensions:\n",
        "       [num_candidates,]. In this array, ties are allowed and there can be\n",
        "       multiple potential winners.\n",
        "     - Untied aggregated social rank. The ranks are untied using the\n",
        "       `tie_breaking_method`. If there were no ties in the social ranking,\n",
        "       this just returns the same ranking.\n",
        "   \"\"\"\n",
        "   rng = np.random.default_rng(seed)\n",
        "\n",
        "   # TODO(miba): Add test for mock rankings.\n",
        "   rankings = filter_out_mocks(rankings)\n",
        "   if rankings.size == 0:\n",
        "     social_ranking_with_ties = np.full(\n",
        "         (rankings.shape[1]), RANKING_MOCK, dtype=np.int32)\n",
        "     social_ranking_without_ties = rng.permutation(\n",
        "         np.arange(rankings.shape[1])).astype(np.int32)\n",
        "     return SocialRankingResult(\n",
        "         social_ranking_with_ties, social_ranking_without_ties\n",
        "     )\n",
        "   social_ranking = self.aggregate_with_ties(rankings)\n",
        "\n",
        "   # Return if there are no ties or the method is TIES_ALLOWED.\n",
        "   if (\n",
        "       is_untied_ranking(social_ranking)\n",
        "       or self._tie_breaking_method == TieBreakingMethod.TIES_ALLOWED\n",
        "   ):\n",
        "     return SocialRankingResult(social_ranking, social_ranking)\n",
        "   # If not, we need to break the ties.\n",
        "   else:\n",
        "     tied_social_ranking = social_ranking.copy()\n",
        "\n",
        "     # Schulze tie-breaking ranking of the candidates (TBRC).\n",
        "     if self._tie_breaking_method == TieBreakingMethod.TBRC:\n",
        "       # Copy and shuffle rankings so we can randomly select ballots.\n",
        "       random_ballots = rankings.copy()\n",
        "       rng.shuffle(random_ballots)\n",
        "\n",
        "       # Each iteration, we try one random ballot and keep track of already\n",
        "       # untied positions.\n",
        "       for random_ballot in random_ballots:  # Loop over shuffled ballots.\n",
        "         # Untie social ranking with random ballot.\n",
        "         social_ranking = untie_ranking_with_ballot(\n",
        "             social_ranking, random_ballot\n",
        "         )\n",
        "         # Exit the while loop if there are no more ties.\n",
        "         if is_untied_ranking(social_ranking):\n",
        "           return SocialRankingResult(\n",
        "               tied_social_ranking,\n",
        "               social_ranking,\n",
        "           )\n",
        "\n",
        "     # If there are still ties or the method is random, break ties randomly.\n",
        "     if self._tie_breaking_method in [\n",
        "         TieBreakingMethod.RANDOM,\n",
        "         TieBreakingMethod.TBRC,\n",
        "     ]:\n",
        "       # New random ballot that can be added to untie rankings.\n",
        "       random_ballot = np.arange(social_ranking.size)\n",
        "       rng.shuffle(random_ballot)\n",
        "       social_ranking = untie_ranking_with_ballot(\n",
        "           social_ranking, random_ballot\n",
        "       )\n",
        "       return SocialRankingResult(\n",
        "           tied_social_ranking,\n",
        "           social_ranking,\n",
        "       )\n",
        "     raise ValueError(\n",
        "         f'tie_breaking_method {self._tie_breaking_method.name} is not'\n",
        "         ' supported.'\n",
        "     )\n",
        "\n",
        " def aggregate_with_ties(\n",
        "     self,\n",
        "     rankings: np.ndarray,\n",
        " ) -> np.ndarray:\n",
        "   \"\"\"Aggregates rankings into a single social ranking with potential ties.\"\"\"\n",
        "\n",
        "   check_rankings(rankings)\n",
        "\n",
        "   pairwise_defeats = self._compute_pairwise_defeats(rankings)\n",
        "   strongest_path_strengths = self._compute_strongest_path_strengths(\n",
        "       pairwise_defeats)\n",
        "   social_ranking = self._rank_candidates(\n",
        "       strongest_path_strengths)\n",
        "   return social_ranking\n",
        "\n",
        " def _compute_pairwise_defeats(self, rankings: np.ndarray) -> np.ndarray:\n",
        "   \"\"\"Computes the number of votes who prefer one over the other candidate.\n",
        "\n",
        "   Args:\n",
        "     rankings: Array of batched rankings with dimensions: [num_citizens,\n",
        "       num_candidates].\n",
        "\n",
        "   Returns:\n",
        "     An array with the number of voters who prefer candidate x to candidate y.\n",
        "       Dimensions [num_candidates, num_candidates].\n",
        "   \"\"\"\n",
        "   num_citizens, num_candidates = rankings.shape\n",
        "   pairwise_defeats = np.zeros(\n",
        "       (num_candidates, num_candidates), dtype=np.int32)\n",
        "   for citizen_id in range(num_citizens):\n",
        "     for idx in range(num_candidates):\n",
        "       for idy in range(num_candidates):\n",
        "         # Lower is better as the higest rank is 0.\n",
        "         if rankings[citizen_id, idx] < rankings[citizen_id, idy]:\n",
        "           pairwise_defeats[idx, idy] += 1\n",
        "   return pairwise_defeats\n",
        "\n",
        " def _compute_strongest_path_strengths(\n",
        "     self, pairwise_defeats: np.ndarray) -> np.ndarray:\n",
        "   \"\"\"Computes the strength of the strongest path between candidates.\n",
        "\n",
        "   Args:\n",
        "     pairwise_defeats: An array with the number of voters who prefer candidate\n",
        "         x to candidate y. Dimensions [num_candidates, num_candidates].\n",
        "   Returns:\n",
        "     An array with the strength of the strongest path between candidate x and\n",
        "     candidate y. Dimensions [num_candidates, num_candidates].\n",
        "   \"\"\"\n",
        "   if len(set(pairwise_defeats.shape)) != 1:\n",
        "     raise ValueError('pairwise_defeats should be a square array.')\n",
        "   if np.any(np.diag(pairwise_defeats) != 0):\n",
        "     raise ValueError('pairwise_defeats should have an all zero diagonal.')\n",
        "\n",
        "   num_candidates = pairwise_defeats.shape[0]\n",
        "   path_strengths = np.zeros((num_candidates, num_candidates), dtype=np.int32)\n",
        "   for idx in range(num_candidates):\n",
        "     for idy in range(num_candidates):\n",
        "       if idx != idy:\n",
        "         if pairwise_defeats[idx, idy] > pairwise_defeats[idy, idx]:\n",
        "           path_strengths[idx, idy] = pairwise_defeats[idx, idy]\n",
        "\n",
        "   for idx in range(num_candidates):\n",
        "     for idy in range(num_candidates):\n",
        "       if idx != idy:\n",
        "         for idz in range(num_candidates):\n",
        "           if idx != idz and idy != idz:\n",
        "             path_strengths[idy, idz] = max(\n",
        "                 path_strengths[idy, idz],\n",
        "                 min(path_strengths[idy, idx], path_strengths[idx, idz]),\n",
        "             )\n",
        "\n",
        "   return path_strengths\n",
        "\n",
        " def _rank_candidates(self, path_strengths: np.ndarray) -> np.ndarray:\n",
        "   \"\"\"Rank the candidates by winning path strength.\n",
        "\n",
        "   Args:\n",
        "     path_strengths: An array with the strength of the strongest path between\n",
        "       candidate x and candidate y. Dimensions [num_candidates,\n",
        "       num_candidates].\n",
        "   Returns:\n",
        "     An array with the aggregated social rank for each candidate. Dimensions\n",
        "       [num_candidates,]. Note that this social rank can contain ties.\n",
        "   \"\"\"\n",
        "\n",
        "   if len(set(path_strengths.shape)) != 1:\n",
        "     raise ValueError('The path_strengths array should be square.')\n",
        "   if np.any(np.diag(path_strengths) != 0):\n",
        "     raise ValueError('path_strengths should have an all zero diagonal.')\n",
        "\n",
        "   # Compute the margin array and determine pairwise weak preferences.\n",
        "   pairwise_dominance = (path_strengths - path_strengths.T) >= 0\n",
        "\n",
        "   # Potential winners are those are preferred (weakly) most often.\n",
        "   weakly_preferred_count = pairwise_dominance.sum(axis=1)\n",
        "\n",
        "   # We can compute the rankings from the weakly preferred count as the binary\n",
        "   # relationships (A >= B) from Schulze are transitive (see page 200 from\n",
        "   # https://arxiv.org/pdf/1804.02973.pdf).\n",
        "   _, rankings = np.unique(-1 * weakly_preferred_count, return_inverse=True)\n",
        "   return rankings"
      ],
      "metadata": {
        "id": "7iYlOU39Dm_H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Base class for statement models.\n",
        "\n",
        "StatementResult = NamedTuple(\n",
        "   'StatementResult',\n",
        "   [\n",
        "       ('statement', str),\n",
        "       ('explanation', str),\n",
        "   ],\n",
        ")\n",
        "\n",
        "class BaseStatementModel(abc.ABC):\n",
        " \"\"\"Base class for reward models that rank multiple statements.\"\"\"\n",
        "\n",
        " @abc.abstractmethod\n",
        " def generate_statement(\n",
        "     self,\n",
        "     llm_client: LLMClient,\n",
        "     question: str,\n",
        "     opinions: Sequence[str],\n",
        "     previous_winner: str | None = None,\n",
        "     critiques: Sequence[str] | None = None,\n",
        "     seed: int | None = None,\n",
        " ) -> StatementResult:\n",
        "   \"\"\"Samples text from the model.\n",
        "\n",
        "   Args:\n",
        "     llm_client: The LLM client used to generate the statement.\n",
        "     question: Question that the citizens are responding to.\n",
        "     opinions: Text-based opinions of the citizens.\n",
        "     previous_winner: The statement that won the previous round.\n",
        "     critiques: Critiques of the previous winner.\n",
        "     seed: Optional seed for the sampling. If None a random seed will be used.\n",
        "\n",
        "   Returns:\n",
        "     A tuple containing:\n",
        "       - The predicted statement.\n",
        "       - The explanation (e.g. chain-of-thought)\n",
        "   \"\"\"\n",
        "   raise NotImplementedError('generate_statement method is not implemented.')"
      ],
      "metadata": {
        "id": "-HcWFvAoDnEl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utils for social choice methods.\n",
        "\n",
        "class TieBreakingMethod(enum.Enum):\n",
        " \"\"\"Method for breaking ties.\"\"\"\n",
        " TIES_ALLOWED = 'ties_allowed'\n",
        " RANDOM = 'random'\n",
        " TBRC = 'tbrc'  # Schulze tie-breaking ranking of the candidates (TBRC).\n",
        "\n",
        "def filter_out_mocks(rankings: np.ndarray) -> np.ndarray:\n",
        " \"\"\"Filters out mock rankings and checks whether mocks are correctly used.\"\"\"\n",
        " if not np.issubdtype(rankings.dtype, np.integer):\n",
        "   raise ValueError(\n",
        "       f'The array should be an integer array but is {rankings.dtype}.'\n",
        "   )\n",
        " is_mock = rankings == RANKING_MOCK\n",
        " any_mock = is_mock.any(axis=1)\n",
        " all_mock = is_mock.all(axis=1)\n",
        " if not (any_mock == all_mock).all():\n",
        "   raise ValueError(\n",
        "       'If a citizen uses a mock rank for one candidate'\n",
        "       'it should use a mock rank for all candidates.'\n",
        "   )\n",
        " return rankings[np.logical_not(any_mock)]\n",
        "\n",
        "\n",
        "def normalize_ranking(ranking: np.ndarray) -> np.ndarray:\n",
        " \"\"\"Normalizes ranking so e.g. [0, 2, 5, 5] -> [0, 1, 2, 2].\"\"\"\n",
        " if ranking.ndim != 1:\n",
        "   raise ValueError('The input array should be a single ranking so `ndim=1`')\n",
        " _, normalized_ranking = np.unique(ranking, return_inverse=True)\n",
        " return normalized_ranking\n",
        "\n",
        "\n",
        "def is_untied_ranking(ranking: np.ndarray) -> bool:\n",
        " \"\"\"Checks if the ranking is untied.\"\"\"\n",
        " if ranking.ndim != 1:\n",
        "   raise ValueError('The input array should be a single ranking so `ndim=1`')\n",
        " return np.unique(ranking).size == ranking.size\n",
        "\n",
        "\n",
        "def check_rankings(rankings: np.ndarray, allow_ties: bool = True) -> None:\n",
        " \"\"\"Checks if a ranking array is a valid ranking array.\n",
        "\n",
        " Args:\n",
        "   rankings: Array of batched rankings with dimensions: [num_citizens,\n",
        "       num_candidates]. In this array lower is better and the best candidate\n",
        "       is thus given rank 0. For example, an array [[1, 0], [0, 0]] corresponds\n",
        "       to the first citizen preferring candidate 1 over candidate 0 while the\n",
        "       second citizen has no preference for either candidate over the other.\n",
        "       We assume that the mock ranks have been filtered out.\n",
        "   allow_ties: If True, rating ties are allowed.\n",
        " \"\"\"\n",
        " if not np.issubdtype(rankings.dtype, np.integer):\n",
        "   raise ValueError(\n",
        "       f'The array should be an integer array but is {rankings.dtype}.')\n",
        "\n",
        " sorted_rankings = np.sort(rankings, axis=1)\n",
        " if np.any(sorted_rankings[:, 0] != 0):\n",
        "   raise ValueError('All rankings should have a 0 as highest ranking')\n",
        "\n",
        " diff_sorted_rankings = np.diff(sorted_rankings, axis=1)\n",
        "\n",
        " if allow_ties:\n",
        "   if not np.all(\n",
        "       np.logical_or(diff_sorted_rankings == 1, diff_sorted_rankings == 0)):\n",
        "     raise ValueError(\n",
        "         'Incorrect ratings, the step size between ratings should be 0 or 1.')\n",
        " else:\n",
        "   if not np.all(diff_sorted_rankings == 1):\n",
        "     raise ValueError('Incorrect ratings, the step size between ratings should'\n",
        "                      ' be 1 as ties are not allowed.')\n",
        "\n",
        "\n",
        "def untie_ranking_with_ballot(\n",
        "   ranking: np.ndarray, ballot: np.ndarray) -> np.ndarray:\n",
        " \"\"\"Unties ranking with extra ballot and renormalizes rankings.\"\"\"\n",
        " if ranking.ndim != 1:\n",
        "   raise ValueError('The input array should be a single ranking so `ndim=1`')\n",
        " if ranking.shape != ballot.shape:\n",
        "   raise ValueError('The ranking and ballot should have the same shape.')\n",
        " # We multiply the rankings with the number of candidates to ensure that we do\n",
        " # not change the order of the already sorted candidates. We then add a ballot\n",
        " # to untie the social ranking.\n",
        " ranking = normalize_ranking(\n",
        "     ranking) * len(ranking) + normalize_ranking(ballot)\n",
        "\n",
        " # Renormalize the social ranking to make sure ranks are consecutive.\n",
        " return normalize_ranking(ranking)"
      ],
      "metadata": {
        "id": "6LDPwj48DnHd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [HAS PROMPT] A model that uses the chain-of-thought method to generate statements.\n",
        "\n",
        "class COTModel(BaseStatementModel):\n",
        " \"\"\"Statement model that uses chain-of-thought reasoning.\"\"\"\n",
        "\n",
        " def generate_statement(\n",
        "     self,\n",
        "     llm_client: LLMClient,\n",
        "     question: str,\n",
        "     opinions: Sequence[str],\n",
        "     previous_winner: str | None = None,\n",
        "     critiques: Sequence[str] | None = None,\n",
        "     seed: int | None = None,\n",
        "     override_prompt: str | None = None,\n",
        "     retries_if_empty: int = 5,\n",
        " ) -> StatementResult:\n",
        "   \"\"\"Generates a statement (see base model).\"\"\"\n",
        "   for i in range(retries_if_empty):\n",
        "    prompt = _generate_gm_prompt(question, opinions, previous_winner, critiques)\n",
        "    response = llm_client.sample_text(\n",
        "        prompt, terminators=['</answer>'], seed=seed)\n",
        "\n",
        "    statement, explanation = _process_gm_model_response(response)\n",
        "    if len(statement) > 5 and explanation != 'INCORRECT_TEMPLATE':\n",
        "      break\n",
        "   return StatementResult(statement, explanation)\n",
        "\n",
        "def _generate_gm_opinion_critique_prompt(\n",
        "   question: str,\n",
        "   opinions: Sequence[str],\n",
        "   previous_winner: str,\n",
        "   critiques: Sequence[str],\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt using opinions, previous winner, and critiques.\"\"\"\n",
        "\n",
        " prompt = f\"\"\"You are assisting a citizens' jury in forming a group jury opinion on an important question. The jury members have provided their individual opinions, a first draft of a jury statement was created, and critiques of that draft were gathered. Your role is to generate a revised group jury statement that incorporates the feedback and aims to better represent the collective view of the jury. Do not make large edits unless several citizens ask for large edits. Stick to the original proposal and respond surgically to the feedback.\n",
        "\n",
        "Please think through this task step-by-step:\n",
        "\n",
        "1. Carefully analyze the individual opinions, noting key themes, points of agreement, and areas of disagreement.\n",
        "2. Review the previous draft group jury statement and identify its strengths and weaknesses vis a vis the critiques.\n",
        "3. Analyze the critiques of the previous draft, paying attention to specific suggestions and concerns raised by the jury members. Analyze this both with respect to the opinion expressed in the statement as well as the specific proposal(s) in the draft statement.\n",
        "4. Based on the opinions, the previous draft, and the critiques, revise the group statement, creating a revised group jury statement that addresses the concerns raised and better reflects the collective view of the jury and an actionable, agreeable plan. Ensure the statement is clear, concise, addresses the core issue posed in the question, and appeals to the group to the maximum extent possible.  Do not mention specific opinion and critique numbers when making your revisions.\n",
        "\n",
        "Provide your answer in the following format:\n",
        "<answer>\n",
        "[Your step-by-step reasoning and explanation for the revised statement]\n",
        "<sep>\n",
        "[Revised consensus statement]\n",
        "</answer>\n",
        "\n",
        "Example:\n",
        "<answer>\n",
        "1. The individual opinions show a general agreement that childcare is important and that parents need support, with several opinions highlighting the financial burden of childcare and the need to enable parents to work (Opinions 1, 3, 4, 5). There is consensus that childcare should support development and learning (Opinions 1, 2, 5) and not just be a childminding service. A point of contention is the age at which universal free childcare should start, with some preferring a later start to allow for bonding (Opinion 1) and others wanting childcare from birth, or the option for parental leave (Opinion 4). There's support for universal paid parental leave for both parents (Opinions 1, 5) and the idea of offering the choice between childcare and paid leave at certain ages (Opinion 1, 5).\n",
        "2. The previous draft statement made some good points, but could not reconcile the differences in the start age of childcare, and was potentially unclear on some issues, such as whether paid childcare from birth was to be offered as an option (not just the default).\n",
        "3. The critiques highlight the need to make a clear distinction between universal free childcare from birth and parental leave. While all agree childcare must enhance development (Critique 2, 5), some wanted to allow parents to access paid childcare from birth if needed (Critique 1), though not as the default. Concerns were raised about the costs to small businesses of paid parental leave (Critique 2), the need for childcare to make going back to work worthwhile, and the need for sufficient pay during parental leave (Critique 4). Critiques also raised the need for universality to be irrespective of circumstance (Critique 3), and a positive reiteration of some aspects of the draft was also given (Critique 5).\n",
        "4. Based on the opinions, the previous draft, and the critiques, the revised statement offers both universal paid parental leave and universal free childcare, but not necessarily from birth, allowing for early bonding between parents and children while also acknowledging the need for working parents to be supported. The revised statement provides a structure for a universal system that addresses multiple family situations without imposing a single approach, recognizing parents can choose paid care from birth if needed but that universal *free* childcare should start later. The statement is clear and concise, addresses the issue of the form and cost of childcare, and is intended to appeal to as many jury members as possible by offering both supports. We also acknowledge the importance of these systems being available to everyone, irrespective of circumstance. To implement this we would propose that the government offer 6 months of parental leave at 80% of the average weekly wage and to implement universal free childcare from 9 months, operating from 8am to 6pm, Monday to Friday. All parents should have the option to access paid childcare from birth, with the government subsiding this childcare at a rate of 20% until the child is 9 months old. This phased approach allows parents flexibility while also moving towards universal childcare.\n",
        "<sep>\n",
        "We agree that it is important to support parents and children through both parental leave and free childcare. We support the government providing universal paid parental leave, and that this should be available to both parents. We also support the provision of universal free childcare, but not necessarily from birth, acknowledging the benefits of bonding in the early months. However, we support parents being able to access paid childcare from birth if they need it. To enable working parents to return to work we propose that free childcare should be provided in a way that supports children's development and learning, not just as a childminding service. These supports should be available to all, irrespective of their family situation. To implement this we would propose that the government offer 6 months of parental leave at 80% of the average weekly wage and to implement universal free childcare from 9 months, operating from 8am to 6pm, Monday to Friday. All parents should have the option to access paid childcare from birth, with the government subsidising this childcare at a rate of 20% until the child is 9 months old.\n",
        "</answer>\n",
        "\n",
        "Below you will find the question, the individual opinions, the previous draft jury statement, and the critiques provided by the jury members.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Individual Opinions:\n",
        "\"\"\"\n",
        " for i, opinion in enumerate(opinions):\n",
        "   prompt += f'Opinion Person {i+1}: {opinion}\\n'\n",
        "\n",
        " prompt += f\"\"\"\n",
        "Previous Draft Jury Statement: {previous_winner}\n",
        "\n",
        "Critiques of the Previous Draft:\n",
        "\"\"\"\n",
        "\n",
        " for i, critique in enumerate(critiques):\n",
        "   prompt += f'Critique Person {i+1}: {critique}\\n'\n",
        "\n",
        " return prompt.strip()\n",
        "\n",
        "def _generate_gm_opinion_only_prompt(\n",
        "   question: str,\n",
        "   opinions: Sequence[str],\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt for the LLM using only the opinions.\"\"\"\n",
        " prompt = f\"\"\"\n",
        "You are assisting a citizens' jury in forming an initial consensus opinion on an important question. The initial consensus opinion should begin with a clear statement of the group's position on the question (e.g., Yes or No, Support or Oppose). It should also include a thorough and detailed justification or argument, based on the jury's opinions that were written and using specific language from the jury's opinions when possible. In essence, the statement should capture the main points of agreement and represent the collective view of the jury but using the specific language from the jury.\n",
        "In addition, consider drafting a concrete proposal for a plan of action. The proposal would be crafted from proposals suggested inside the opinions of the jury members or variants thereof. The proposal should appeal to the group to the maximum extent possible. The proposal should outline a plan for implementation.  While research or studies or task forces may be part of the process, the core of the proposal should be a specific action or set of actions aimed at addressing the issue directly, and not just studying it. If there are numbers involved (like times, ages, or monetary quantities), propose specific numbers. But again, only make a proposal if the group seems ready for it. One sign they are ready for it is if there is substantial agreement already on the issue, or if someone in the jury directly proposes something.\n",
        "\n",
        "Please think through this task step-by-step:\n",
        "\n",
        "1. Carefully analyze the individual opinions, noting key themes, points of agreement, areas of disagreement, and whether or not arguments make specific proposals or are just statements of values or arguments.\n",
        "2. Based on the analysis, synthesize a clear, specific, and actionable jury statement that represents the shared perspective of the jury members.  Address the core issue posed in the question, and ensure the statement appeals to the group members to the maximum extent possible.  Do not refer specific opinion numbers or citizen numbers in the statement, but you may in the analysis.\n",
        "3. Consider adding an additional concrete suggestion based on the opinions submitted. This can be a proposal from one of the jury members or a variant thereof. In the absence of specific proposals, come up a new proposal that would appeal to the most group members as possible. Consider the trade-off of being specific but alienting more people, and err on the side of being more specific and actionable.\n",
        "\n",
        "Provide your answer in the following format:\n",
        "<answer>\n",
        "[Your step-by-step reasoning and explanation for the statement]\n",
        "<sep>\n",
        "[Draft consensus statement]\n",
        "</answer>\n",
        "\n",
        "Example:\n",
        "<answer>\n",
        "1. Most opinions acknowledge the benefits of childcare for parents and children, with support for enabling parents to work and children to develop socially and educationally (Opinions 1, 3, 4, 5). There is a general consensus that support for parents is needed, whether through childcare or paid leave. A key point of divergence is the appropriate age for universal free childcare to begin, with some advocating for a later start (Opinions 1, 2) to allow for bonding in the early months, and some wanting a start from birth, or offering a choice of childcare or stay-at-home income (Opinions 4). Opinions also highlight the need to target childcare towards those who cannot afford to pay (Opinion 2). Some mention the gendered nature of childcare (Opinions 3), with the need to reduce barriers to care for both men and women. There's a strong emphasis on the high cost of childcare impacting families financially (Opinions 3, 4).\n",
        "2. The jury statement prioritizes support for parents through parental leave and free childcare, acknowledging the importance of early childhood development. It suggests a phased approach, with paid parental leave from birth and free childcare from a later age, allowing for both bonding and economic support, which reflects the different perspectives presented. The need for childcare to support both development and learning, and not just function as a childminding service, is also highlighted.\n",
        "3. Building on the desire for support for parents from birth and concrete suggestions by the opinions, we propose a two-pronged approach to start within 3 months. First, the government should establish a universal paid parental leave scheme, offering 6 months of leave at 80% of the parent's average weekly wage, available to both parents and transferable between them. The funds for this scheme should be diverted from current paid childcare schemes. Second, the government should introduce universal free childcare for children from the age of 9 months, operating from 8am to 6pm Monday to Friday, to be implemented nationally at the same time as the parental leave program. To facilitate uptake the city should promote both programs through national campaigns on TV, social media, and in public information centres, especially at family doctor offices, and offer a help line for any queries. This addresses the need for support at birth and offers concrete action on childcare.\n",
        "<sep>\n",
        "In general, free childcare is a good thing, but it is important to consider how it is provided and for which age groups. We feel that it is important to offer support to parents in the form of parental leave, and that this should be available to both parents. In addition, we feel that free childcare should be provided from a young age, and that it should be provided in a way that supports children's development and learning, and not just as a childminding service. However, we do not feel that free childcare should be provided from birth, as we feel that it is important for babies to have a consistent primary caregiver in their early months. For this reason, we would support the government providing universal paid parental leave from birth, and providing universal free childcare from, say, 6 months old. We would also offer parents the opportunity to either use free childcare between 6 months and 1 year, or to have paid parental leave for the same period.\n",
        "</answer>\n",
        "\n",
        "Below you will find the question and the individual opinions of the jury members.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Individual Opinions:\n",
        "\"\"\"\n",
        "\n",
        " for i, opinion in enumerate(opinions):\n",
        "   prompt += f'Opinion Person {i+1}: {opinion}\\n'\n",
        "\n",
        " return prompt.strip()\n",
        "\n",
        "def _generate_gm_prompt(\n",
        "   question: str,\n",
        "   opinions: Sequence[str],\n",
        "   previous_winner: str | None = None,\n",
        "   critiques: Sequence[str] | None = None,\n",
        ") -> str:\n",
        " \"\"\"Generates a prompt for the LLM.\"\"\"\n",
        " if previous_winner is None:\n",
        "   return _generate_gm_opinion_only_prompt(question, opinions)\n",
        " else:\n",
        "   return _generate_gm_opinion_critique_prompt(\n",
        "       question, opinions, previous_winner, critiques\n",
        "   )\n",
        "\n",
        "def _process_gm_model_response(response: str) -> tuple[str, str]:\n",
        " \"\"\"Processes the model's response, extracting the statement and explanation.\n",
        "\n",
        " Args:\n",
        "     response: The raw model response.\n",
        "\n",
        " Returns:\n",
        "     A tuple of (statement, explanation).  If the response format is\n",
        "     incorrect, returns (\"\", \"INCORRECT_TEMPLATE\").\n",
        " \"\"\"\n",
        " match = re.search(\n",
        "     r'<answer>\\s*(.*?)\\s*<sep>\\s*(.*?)\\s*</answer>', response, re.DOTALL\n",
        " )\n",
        " if match:\n",
        "   explanation = match.group(1).strip()\n",
        "   statement = match.group(2).strip()\n",
        "   return statement, explanation\n",
        " else:\n",
        "   return '', 'INCORRECT_TEMPLATE'"
      ],
      "metadata": {
        "id": "bn9vB21vDnNc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top-level utils for Habermas Machine.\n",
        "\n",
        "def numerical_ranking_to_ordinal_text(ranking_array):\n",
        " \"\"\"Converts a numerical ranking array to ordinal text representation.\n",
        "\n",
        " Elements with the same rank are ordered ascendingly by their original index.\n",
        " For example:\n",
        "     * [1, 1, 0, 0] becomes \"3 = 4 > 0 = 1\"\n",
        "     * [1, 2, 2, 0] becomes \"4 > 1 > 2 = 3\"\n",
        "     * [0, 0, 2, 1] becomes \"1 = 2 > 4 > 3\"\n",
        "\n",
        " Args:\n",
        "     ranking_array: The NumPy array of rankings.\n",
        "\n",
        " Returns:\n",
        "     A string representing the arrow ranking or None if input is invalid.\n",
        " \"\"\"\n",
        "\n",
        " if not isinstance(ranking_array, np.ndarray) or not np.issubdtype(\n",
        "     ranking_array.dtype, np.integer\n",
        " ):\n",
        "   raise ValueError(\n",
        "       f\"The array should be an integer array but is {ranking_array.dtype}.\"\n",
        "   )\n",
        "\n",
        " n = len(ranking_array)\n",
        " ranked_elements = sorted(  # Use original indices\n",
        "     zip(ranking_array, range(n)))\n",
        "\n",
        " result = []\n",
        " current_rank = -1  # Initialize to an invalid rank.\n",
        " current_group = []\n",
        "\n",
        " for rank, original_index in ranked_elements:\n",
        "   if rank != current_rank:  # Start a new group\n",
        "     if current_group:  # Append the previous group if it exists\n",
        "       result.append(\n",
        "           # Adjust the rank to start from 1.\n",
        "           \" = \".join(str(i + 1) for i in sorted(current_group)))\n",
        "     current_rank = rank\n",
        "     current_group = [original_index]\n",
        "   else:\n",
        "     current_group.append(original_index)\n",
        "\n",
        " result.append(\" = \".join(str(i + 1) for i in sorted(current_group)))\n",
        "\n",
        " return \" > \".join(result)"
      ],
      "metadata": {
        "id": "FMGa0GsrFAsz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Habermas Machine.\n",
        "\n",
        "class HabermasMachine:\n",
        " \"\"\"Mediates caucus deliberation among participants.\n",
        "\n",
        " The Habermas Machine facilitates AI-mediated deliberation among a group of\n",
        " participants on a given question. It acts as a \"mediator,\" iteratively\n",
        " refining a group statement that aims to capture the common ground of the\n",
        " participants' opinions.\n",
        "\n",
        " The process involves:\n",
        "\n",
        " 1. Gathering initial opinions from participants.\n",
        " 2. Generating candidate group statements using a Large Language Model (LLM).\n",
        " 3. Evaluating these statements using a personalized reward model, predicting\n",
        "    the order of preference of each participant for each statement.\n",
        " 4. Aggregating individual preferences using a social choice method to select\n",
        "    a winning statement.\n",
        " 5. Gathering critiques of the winning statement from participants.\n",
        " 6. Generating revised statements based on the critiques and previous opinions.\n",
        " 7. Optionally, repeating steps 3-6 for multiple rounds, refining the statement\n",
        "    iteratively. In the paper, we use one opinion and one critique round.\n",
        "\n",
        " This class manages the entire deliberation process, including interaction with\n",
        " the LLM, the reward model, and the social choice mechanism.  It maintains the\n",
        " history of opinions, critiques, candidate statements, and winning statements\n",
        " for each round.\n",
        " \"\"\"\n",
        "\n",
        " def __init__(\n",
        "     self,\n",
        "     question: str,\n",
        "     statement_client: LLMClient,\n",
        "     reward_client: LLMClient,\n",
        "     statement_model: BaseStatementModel,\n",
        "     reward_model: BaseRankingModel,\n",
        "     social_choice_method: Base,\n",
        "     num_candidates: int = 16,\n",
        "     num_citizens: int = 5,\n",
        "     seed: int | None = None,\n",
        "     verbose: bool = False,\n",
        " ):\n",
        "   \"\"\"Initializes the Habermas Machine.\"\"\"\n",
        "   self._question = question  # Question to be answered.\n",
        "   self._round = 0  # Current round (round 0 is the opinion round).\n",
        "   self._critiques = []  # Critiques from current and previous rounds.\n",
        "   self._statement_client = statement_client\n",
        "   self._reward_client = reward_client\n",
        "   self._statement_model = statement_model\n",
        "   self._reward_model = reward_model\n",
        "   self._social_choice_method = social_choice_method\n",
        "   self._num_candidates = num_candidates  # Number of candidates to generate.\n",
        "   self._rng = np.random.default_rng(seed)  # Random number generator.\n",
        "   self._num_citizens = num_citizens\n",
        "   self._previous_winners = []  # Winning statements from previous rounds.\n",
        "   self._ranking_explanations = []  # Explanations for the rankings.\n",
        "   self._previous_tied_rankings = []  # Rankings from previous rounds.\n",
        "   self._previous_untied_rankings = []  # Untied rankings from previous rounds.\n",
        "   self._statement_explanations = []  # Explanations for the statements.\n",
        "   self._previous_candidates = []  # Candidates from previous rounds.\n",
        "   self._verbose = verbose  # Whether to print round information.\n",
        "   self._opinions = []  # Initial opinions.\n",
        "\n",
        " def _get_new_seed(self):\n",
        "   \"\"\"Generates a new random seed.\"\"\"\n",
        "   return self._rng.integers(np.iinfo(np.int32).max)\n",
        "\n",
        "\n",
        " def _generate_statements(\n",
        "     self,\n",
        " ) -> tuple[list[str], list[str]]:  # statements, explanations.\n",
        "   \"\"\"Generates candidate statements.\"\"\"\n",
        "   statements = []\n",
        "   explanations = []\n",
        "   for _ in range(self._num_candidates):\n",
        "     # Shuffle the opinions and critiques to avoid ordering bias.\n",
        "     indices = self._rng.permutation(self._num_citizens)\n",
        "     shuffled_opinions = [self._opinions[j] for j in indices]\n",
        "     shuffled_critiques = (\n",
        "         [self._critiques[-1][i] for i in indices] if self._critiques else None\n",
        "     )\n",
        "     statement, explanation = self._statement_model.generate_statement(\n",
        "         llm_client=self._statement_client,\n",
        "         question=self._question,\n",
        "         opinions=shuffled_opinions,\n",
        "         previous_winner=(\n",
        "             self._previous_winners[-1] if self._previous_winners else None),\n",
        "         critiques=shuffled_critiques,\n",
        "         seed=self._get_new_seed(),\n",
        "     )\n",
        "     statements.append(statement)\n",
        "     explanations.append(explanation)\n",
        "   return statements, explanations\n",
        "\n",
        "\n",
        " def _get_rankings(\n",
        "     self, statements: list[str]) -> tuple[np.ndarray, list[None | str]]:\n",
        "   \"\"\"Gets rankings over all candidates for each citizen.\"\"\"\n",
        "   all_rankings = []\n",
        "   explanations = []\n",
        "   for i in range(self._num_citizens):\n",
        "     # Shuffle the statements to avoid ordering bias.\n",
        "     indices = self._rng.permutation(self._num_candidates)\n",
        "     shuffled_statements = [statements[j] for j in indices]\n",
        "\n",
        "     ranking, explanation = self._reward_model.predict_ranking(\n",
        "         llm_client=self._reward_client,\n",
        "         question=self._question,\n",
        "         opinion=self._opinions[i],\n",
        "         statements=shuffled_statements,\n",
        "         previous_winner=(\n",
        "             self._previous_winners[-1] if self._round > 0 else None\n",
        "         ),\n",
        "         critique=self._critiques[-1][i] if self._round > 0 else None,\n",
        "     )\n",
        "\n",
        "     if ranking is None:\n",
        "       raise ValueError(\n",
        "           f\"Ranking is None for citizen {i+1}. Explanation: {explanation}\")\n",
        "\n",
        "     unshuffled_ranking = np.full_like(ranking, fill_value=RANKING_MOCK)\n",
        "     unshuffled_ranking[indices] = ranking\n",
        "     all_rankings.append(unshuffled_ranking)\n",
        "     explanations.append(explanation)\n",
        "   return np.array(all_rankings), explanations\n",
        "\n",
        " def overwrite_previous_winner(self, winner: str):\n",
        "   \"\"\"Overwrites the last winner.\"\"\"\n",
        "   if self._round == 0:\n",
        "     raise ValueError(\"There is no previous winner before the opinion round.\")\n",
        "   else:\n",
        "     if self._verbose:\n",
        "       print(\"\\nOverwriting last winner.\")\n",
        "       print(f\"Previous winner: {self._previous_winners[-1]}\")\n",
        "       print(f\"New winner: {winner}\")\n",
        "     self._previous_winners[-1] = winner\n",
        "\n",
        " def mediate(\n",
        "     self, opinions_or_critiques: Sequence[str]) -> tuple[str, list[str]]:\n",
        "   \"\"\"Runs a single medatiation step and returns the winning statement.\"\"\"\n",
        "   if len(opinions_or_critiques) != self._num_citizens:\n",
        "     raise ValueError(\n",
        "         f\"Expected {self._num_citizens} opinions or critiques, got\"\n",
        "         f\" {len(opinions_or_critiques)}.\"\n",
        "     )\n",
        "\n",
        "   if self._round == 0:\n",
        "     self._opinions = list(opinions_or_critiques)\n",
        "   else:\n",
        "     self._critiques.append(list(opinions_or_critiques))\n",
        "\n",
        "   if self._verbose:\n",
        "     if self._round == 0:\n",
        "       print(\"\\n\\nOpinion round.\")\n",
        "     else:\n",
        "       print(f\"\\n\\nCritique round {self._round}.\")\n",
        "     print(f\"\\nQuestion: {self._question}\")\n",
        "     print(\"\\nOpinions:\")\n",
        "     for i, opinion in enumerate(self._opinions):\n",
        "       print(f\"\\tCitizen {i + 1}: {opinion}\")\n",
        "     if self._round > 0:\n",
        "       print(f\"\\nPrevious winner: {self._previous_winners[-1]}\")\n",
        "       print(\"\\nCritiques:\")\n",
        "       for i, critique in enumerate(self._critiques[-1]):\n",
        "         print(f\"\\tCitizen {i + 1}: {critique}\")\n",
        "\n",
        "   statements, statement_explanations = self._generate_statements()\n",
        "\n",
        "   if self._verbose:\n",
        "     print(\"\\nStatements generated:\")\n",
        "     for i, statement in enumerate(statements):\n",
        "       print(f\"\\tStatement {i+1}: {statement}\")\n",
        "\n",
        "   all_rankings, ranking_explanations = self._get_rankings(statements)\n",
        "   if self._verbose:\n",
        "     print(\"\\nRankings:\")\n",
        "     for i, ranking in enumerate(all_rankings):\n",
        "       print(\n",
        "           f\"\\tCitizen {i + 1}:\"\n",
        "           f\" {numerical_ranking_to_ordinal_text(ranking)}\"\n",
        "       )\n",
        "\n",
        "   tied_social_ranking, untied_social_ranking = (\n",
        "       self._social_choice_method.aggregate(\n",
        "           all_rankings, seed=self._get_new_seed()\n",
        "       )\n",
        "   )\n",
        "\n",
        "   if self._verbose:\n",
        "     print(\"\\nPotentially tied social ranking:\")\n",
        "     print(numerical_ranking_to_ordinal_text(tied_social_ranking))\n",
        "     print(\"\\nUntied social ranking:\")\n",
        "     print(numerical_ranking_to_ordinal_text(untied_social_ranking))\n",
        "\n",
        "   # Record the statements with tied and untied rankings.\n",
        "   statements_with_tied_rankings = []\n",
        "   statements_with_untied_rankings = []\n",
        "   for idx, statement in enumerate(statements):\n",
        "     statements_with_untied_rankings.append((\n",
        "         statement,\n",
        "         untied_social_ranking[idx],\n",
        "     ))\n",
        "     statements_with_tied_rankings.append((\n",
        "         statement,\n",
        "         tied_social_ranking[idx],\n",
        "     ))\n",
        "   self._previous_tied_rankings.append(statements_with_tied_rankings)\n",
        "   self._previous_untied_rankings.append(statements_with_untied_rankings)\n",
        "\n",
        "   # Get the sorted indices based on the social_ranking.\n",
        "   sorted_indices = np.argsort(untied_social_ranking)\n",
        "\n",
        "   # Reorder the statements based on the sorted indices.\n",
        "   sorted_statements = [statements[i] for i in sorted_indices]\n",
        "   sorted_statement_explanations = [\n",
        "       statement_explanations[i] for i in sorted_indices\n",
        "   ]\n",
        "   winner = sorted_statements[0]\n",
        "   self._ranking_explanations.append(ranking_explanations)\n",
        "   self._statement_explanations.append(sorted_statement_explanations)\n",
        "   self._previous_winners.append(winner)\n",
        "   self._previous_candidates.append(sorted_statements)\n",
        "\n",
        "   if self._verbose:\n",
        "     print(f\"\\nWinning statement: {winner}\")\n",
        "\n",
        "   self._round += 1\n",
        "   return winner, sorted_statements"
      ],
      "metadata": {
        "id": "5KGJPgSeEtek",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions for names, opinions, and critiques\n",
        "\n",
        "def get_number_of_people():\n",
        "    \"\"\"Gets a specific number of names and numbers them.\"\"\"\n",
        "    return int(input(f\"How many people are in the group? \"))\n",
        "\n",
        "def get_and_number_names(num_names):\n",
        "    \"\"\"Gets a specific number of names and numbers them.\"\"\"\n",
        "    names = []\n",
        "    for i in range(num_names):\n",
        "        name = input(f\"Enter name {i + 1}: \")\n",
        "        names.append(name)\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def print_numbered_names(name_list):\n",
        "  \"\"\"Prints the list of names numbered\"\"\"\n",
        "  if name_list:\n",
        "    print(\"\\nThe names you entered are:\")\n",
        "    for i, name in enumerate(name_list):\n",
        "      print(f\"Citizen {i+1}: {name}\")\n",
        "\n",
        "\n",
        "def get_opinions(question, names):\n",
        "  \"\"\"Get opinions for the list of names\"\"\"\n",
        "  opinions = [] # Use a dictionary to store name:opinion pairs\n",
        "  print(f'The question is: {question}')\n",
        "  for name in names:\n",
        "    opinion = input(f\"What is {name}'s opinion? \")\n",
        "    opinions.append(opinion) # Store the opinion using name as key\n",
        "  return opinions\n",
        "\n",
        "\n",
        "def print_numbered_opinions(opinions, names):\n",
        "  \"\"\"Prints the opinions in a numbered list\"\"\"\n",
        "  if opinions:\n",
        "    print(\"\\nThe opinions you entered are:\")\n",
        "    for i, (name, opinion) in enumerate(zip(names, opinions)):\n",
        "      print(f\"\\nCitizen {i + 1} ({name}): {opinion}\")\n",
        "\n",
        "\n",
        "def get_critiques(question, statement, names):\n",
        "  \"\"\"Get critiques for the list of names\"\"\"\n",
        "  print(f'The question is: {question}')\n",
        "  print(f'The initial winning statement is: {statement}')\n",
        "  critiques = [] # Use a dictionary to store name:critique pairs\n",
        "  for name in names:\n",
        "    critique = input(f\"What is {name}'s critique? \")\n",
        "    critiques.append(critique)\n",
        "  return critiques\n",
        "\n",
        "\n",
        "def print_numbered_critiques(critiques, names):\n",
        "  \"\"\"Prints the critiques in a numbered list\"\"\"\n",
        "  if critiques:\n",
        "    print(\"\\nThe critiques you entered are:\")\n",
        "    for i, (name, critique) in enumerate(zip(names, critiques)):\n",
        "      print(f\"\\nCitizen {i + 1} ({name}): {critique}\")"
      ],
      "metadata": {
        "id": "NsQnXZZVNsnr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Nice statement printing\n",
        "\n",
        "def nice_statement_printing(statement, width, title=\"Winning statement:\"):\n",
        "    \"\"\"Nice statement printing, Markdown first, then wrapping.\"\"\"\n",
        "\n",
        "    # Create the markdown string first\n",
        "    markdown_string = f\"**{title}**\\n\\n```\\n{statement}\\n```\"\n",
        "\n",
        "    # Render the Markdown to HTML\n",
        "    rendered_markdown = Markdown(markdown_string)\n",
        "\n",
        "    # Extract the raw HTML text\n",
        "    raw_html = rendered_markdown.data  # Access raw HTML of the Markdown object\n",
        "\n",
        "    # Wrap the rendered HTML, ensuring that we don't affect html elements by wrapping\n",
        "    wrapped_html = textwrap.fill(raw_html, width=width)\n",
        "\n",
        "    # Re-display it by wrapping the raw HTML from the original markdown\n",
        "    display(Markdown(wrapped_html)) # Re display with the wrapped html"
      ],
      "metadata": {
        "id": "F2FfJHWNQlo6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MmXHjEgg-r"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlYC-M2t-_Tb"
      },
      "outputs": [],
      "source": [
        "# @title Set API key\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyDK5Cws4LjSQHs3ZnXZmnfZ3B11fTtA9xg'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @title Set model, candidates, and methods\n",
        "\n",
        "NUM_CANDIDATES = 5\n",
        "MODEL = 'gemini-1.5-pro'\n",
        "\n",
        "statement_client = AIStudioClient(model_name=MODEL)\n",
        "reward_client = AIStudioClient(model_name=MODEL)\n",
        "\n",
        "statement_model = COTModel()\n",
        "reward_model = COTRankingModel()\n",
        "social_choice_method = Schulze(\n",
        "    tie_breaking_method=TieBreakingMethod.TBRC\n",
        ")"
      ],
      "metadata": {
        "id": "04Z1QIk8LQMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set model, candidates, and methods\n",
        "\n",
        "NUM_CANDIDATES = 5\n",
        "MODEL = 'gemini-1.5-pro'\n",
        "\n",
        "statement_client = AIStudioClient(model_name=MODEL)\n",
        "reward_client = AIStudioClient(model_name=MODEL)\n",
        "\n",
        "statement_model = COTModel()\n",
        "reward_model = COTRankingModel()\n",
        "social_choice_method = Schulze(\n",
        "    tie_breaking_method=TieBreakingMethod.TBRC\n",
        ")"
      ],
      "metadata": {
        "id": "Gc2VSvya8dt-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "KHGL9MHHLdUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select Question and set up group (if this doesn't immediately query you for the group size, stop and retry)\n",
        "QUESTION_1 = 'Should the UK government prioritize policies that actively encourage AI development in key industries, even if it means potentially delaying significant investment in addressing climate change?' # @param [ 'Should the UK government prioritize policies that actively encourage AI development in key industries, even if it means potentially delaying significant investment in addressing climate change?', 'Should the UK introduce a Universal Basic Income (UBI) for all citizens, regardless of their employment status?']\n",
        "\n",
        "num_citizens = get_number_of_people()\n",
        "names = get_and_number_names(num_citizens)\n",
        "print_numbered_names(names)"
      ],
      "metadata": {
        "id": "U57WqONCLcg0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize the Habermas Machine\n",
        "hm_1 = HabermasMachine(\n",
        "       question=QUESTION_1,\n",
        "       statement_client=statement_client,\n",
        "       reward_client=reward_client,\n",
        "       statement_model=statement_model,\n",
        "       reward_model=reward_model,\n",
        "       social_choice_method=social_choice_method,\n",
        "       num_candidates=NUM_CANDIDATES,\n",
        "       num_citizens=num_citizens,\n",
        "       verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "p9mjom97K99_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get opinions\n",
        "opinions = get_opinions(QUESTION_1, names)\n",
        "print_numbered_opinions(opinions, names)"
      ],
      "metadata": {
        "id": "IOq97Z3ILruK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate initial statement (~2 min).\n",
        "\n",
        "winner, _ = hm_1.mediate(opinions)\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "nice_statement_printing(winner, 100, \"Initial winner:\")"
      ],
      "metadata": {
        "id": "3OXad7pOKUe8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get critiques\n",
        "\n",
        "critiques = get_critiques(QUESTION_1, winner, names)\n",
        "print_numbered_critiques(critiques, names)"
      ],
      "metadata": {
        "id": "gjKsacnLV56w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate revised statement (~2 min).\n",
        "\n",
        "winner, _  = hm_1.mediate(critiques)\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "nice_statement_printing(winner, 100, \"Final winner:\")"
      ],
      "metadata": {
        "id": "CVKHHzuTKVt7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "_nneCq7bFc3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select Question and set up group (if this doesn't immediately query you for the group size, stop and retry)\n",
        "QUESTION_2 = 'Should the UK introduce a Universal Basic Income (UBI) for all citizens, regardless of their employment status?' # @param [ 'Should the UK government prioritize policies that actively encourage AI development in key industries, even if it means potentially delaying significant investment in addressing climate change?', 'Should the UK introduce a Universal Basic Income (UBI) for all citizens, regardless of their employment status?']\n",
        "\n",
        "num_citizens = get_number_of_people()\n",
        "names = get_and_number_names(num_citizens)\n",
        "print_numbered_names(names)"
      ],
      "metadata": {
        "id": "kEPqfxIlWH4-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize the Habermas Machine\n",
        "hm_2 = HabermasMachine(\n",
        "       question=QUESTION_2,\n",
        "       statement_client=statement_client,\n",
        "       reward_client=reward_client,\n",
        "       statement_model=statement_model,\n",
        "       reward_model=reward_model,\n",
        "       social_choice_method=social_choice_method,\n",
        "       num_candidates=NUM_CANDIDATES,\n",
        "       num_citizens=num_citizens,\n",
        "       verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yx5HnQRpFiOv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get opinions\n",
        "opinions_2 = get_opinions(QUESTION_2, names)\n",
        "print_numbered_opinions(opinions_2, names)"
      ],
      "metadata": {
        "id": "nICYj9-iFjmp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate initial statement.\n",
        "\n",
        "winner_2, _ = hm_2.mediate(opinions_2)\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "nice_statement_printing(winner_2, 100, \"Initial winner:\")"
      ],
      "metadata": {
        "id": "HMQiz999FreW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get critiques\n",
        "critiques_2 = get_critiques(QUESTION_2, winner_2, names)\n",
        "print_numbered_critiques(critiques_2, names)"
      ],
      "metadata": {
        "id": "ropfbn1-FvSQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate revised statement.\n",
        "\n",
        "winner_2, _  = hm_2.mediate(critiques_2)\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "nice_statement_printing(winner_2, 100, \"Final winner:\")"
      ],
      "metadata": {
        "id": "7eLqNBmxF6be",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# License and disclaimer\n",
        "\n",
        "Copyright 2024 DeepMind Technologies Limited\n",
        "\n",
        "All software is licensed under the Apache License, Version 2.0 (Apache 2.0);\n",
        "you may not use this file except in compliance with the Apache 2.0 license.\n",
        "You may obtain a copy of the Apache 2.0 license at:\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "All other materials are licensed under the Creative Commons Attribution 4.0\n",
        "International License (CC-BY). You may obtain a copy of the CC-BY license at:\n",
        "https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, all software and\n",
        "materials distributed here under the Apache 2.0 or CC-BY licenses are\n",
        "distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
        "either express or implied. See the licenses for the specific language governing\n",
        "permissions and limitations under those licenses.\n",
        "\n",
        "This is not an official Google product."
      ],
      "metadata": {
        "id": "ONlJ7US4Go9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use or build on this code, please cite: Tessler, M. H., Bakker, M. A., Jarrett D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, J., Collins, T., Parkes, D. C., Botvinick, M., and Summerfield, C. \"AI can help humans find common ground in democratic deliberation.\" *Science*. (2024)."
      ],
      "metadata": {
        "id": "lHe7B1mAH6lY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "5eRjrkiagYWh",
        "w9MmXHjEgg-r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}